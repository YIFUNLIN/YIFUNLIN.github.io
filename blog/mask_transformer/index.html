<!DOCTYPE html>
<html lang="en-us"><head>
  <meta charset="utf-8">
  <title>林羿帆-個人網站</title>

  <!-- mobile responsive meta -->
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="目前在LINE Taiwan 擔任 Backend Engineer Intern">
  <meta name="author" content="Evan Lin">
    
  
  <meta name="theme-name" content="liva-hugo" />
  
  <meta name="generator" content="Hugo 0.148.1">

  <!-- plugins -->
  
  <link rel="stylesheet" href="https://yifunlin.github.io/plugins/bootstrap/bootstrap.min.css ">
  
  <link rel="stylesheet" href="https://yifunlin.github.io/plugins/slick/slick.css ">
  
  <link rel="stylesheet" href="https://yifunlin.github.io/plugins/themify-icons/themify-icons.css ">
  
  <link rel="stylesheet" href="https://yifunlin.github.io/plugins/venobox/venobox.css ">
  

  <!-- Main Stylesheet -->
  
  <link rel="stylesheet" href="https://yifunlin.github.io/scss/style.min.css" media="screen">

  <!--Favicon-->
  <link rel="shortcut icon" href="https://yifunlin.github.io/images/favicon.png " type="image/x-icon">
  <link rel="icon" href="https://yifunlin.github.io/images/favicon.png " type="image/x-icon">

  <!-- google analitycs -->
  <script>
    (function (i, s, o, g, r, a, m) {
      i['GoogleAnalyticsObject'] = r;
      i[r] = i[r] || function () {
        (i[r].q = i[r].q || []).push(arguments)
      }, i[r].l = 1 * new Date();
      a = s.createElement(o),
        m = s.getElementsByTagName(o)[0];
      a.async = 1;
      a.src = g;
      m.parentNode.insertBefore(a, m)
    })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');
    ga('create', '', 'auto');
    ga('send', 'pageview');
  </script>


<script async src="https://www.googletagmanager.com/gtag/js?id=G-K2QY5YMF2Z"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-K2QY5YMF2Z');
</script>

</head>
<body>
<!-- preloader start -->
<div class="preloader">
  
</div>
<!-- preloader end -->
<!-- navigation -->
<header class="navigation">
  <div class="container">
    
    <nav class="navbar navbar-expand-lg navbar-white bg-transparent border-bottom pl-0">
      <a class="navbar-brand mobile-view" href="https://yifunlin.github.io/">
          
































  

  
  


  


        </a>
      <button class="navbar-toggler border-0" type="button" data-toggle="collapse" data-target="#navigation">
        <i class="ti-menu"></i>
      </button>

      <div class="collapse navbar-collapse text-center" id="navigation">
        <div class="desktop-view">
          <ul class="navbar-nav mr-auto">
            
            <li class="nav-item">
              <a class="nav-link" href="https://www.instagram.com/yifan00802/"><i class="ti-instagram"></i></a>
            </li>
            
            <li class="nav-item">
              <a class="nav-link" href="https://github.com/YIFUNLIN"><i class="ti-github"></i></a>
            </li>
            
            <li class="nav-item">
              <a class="nav-link" href="https://www.linkedin.com/in/lin-yi-fan-293082257/"><i class="ti-linkedin"></i></a>
            </li>
            
          </ul>
        </div>

        <a class="navbar-brand mx-auto desktop-view" href="https://yifunlin.github.io/">
            
































  

  
  


  


          </a>

        <ul class="navbar-nav">
          
          
          <li class="nav-item">
            <a class="nav-link" href="https://yifunlin.github.io/about/">About</a>
          </li>
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="https://yifunlin.github.io/blog/">Post</a>
          </li>
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="https://yifunlin.github.io/contact/">Contact</a>
          </li>
          
          
        </ul>

        
        <!-- search -->
        <div class="search pl-lg-4">
          <button id="searchOpen" class="search-btn"><i class="ti-search"></i></button>
          <div class="search-wrapper">
            <form action="https://yifunlin.github.io//search" class="h-100">
              <input class="search-box px-4" id="search-query" name="s" type="search" placeholder="Type & Hit Enter...">
            </form>
            <button id="searchClose" class="search-close"><i class="ti-close text-dark"></i></button>
          </div>
        </div>
        

        
      </div>
    </nav>
  </div>
</header>
<!-- /navigation -->

<section class="section-sm">
  <div class="container">
    <div class="row">
      <div class="col-lg-10 mx-auto">
        
        <a href="/categories/ai"
          class="text-primary">Ai</a>
        
        <h2>Masked-attention Mask Transformer for Universal Image Segmentation (CVPR 2022)</h2>
        <div class="mb-3 post-meta">
          <span>By Evan Lin</span>
          
          <span class="border-bottom border-primary px-2 mx-1"></span>
          <span>06 November 2024</span>
          
        </div>
        
        <div class="content mb-5">
          <h4 id="簡介">簡介：</h4>
<p>分享一下，在資工所修課時，所報告的 paper</p>
<p>Medium 完整版：</p>
<p><a href="https://medium.com/@drose01rrr/masked-attention-mask-transformer-for-universal-image-segmentation-e14f305e433e">https://medium.com/@drose01rrr/masked-attention-mask-transformer-for-universal-image-segmentation-e14f305e433e</a></p>
<hr>
<p>[CV-1] Masked-attention Mask Transformer for Universal Image Segmentation
(CVPR 2022)
Paper Link
Github</p>
<p>接下來這兩年應該都會在Lab 報完paper後，順便寫個medium來記錄一下
不然我應該很快就忘記之前看過啥啦哈哈哈
Abstract
圖像分割(Image segmentation)就是將pixel 進行分組，會因為分組完的語意差異(eg. category or instance membership 實例歸屬)而產生不同的分割任務。當前的研究仍專注於單一任務就設計一個專用的架構，因此，Meta的研究團隊就提出了一個通用的架構: Mask2Transformer，能應用於任何圖像分割任務(panoptic 全景、instance 實例 or semantic語意)，關鍵就是在於透過改良一般的self-attention機制，將其設計為Mask-attention。透過限制cross-attention 預測的範圍去提取局部特徵，不僅減少了三倍研究的時間，還在四個最熱門的資料集中的表現大幅領現這些專用的架構。
Instance membership: 指的是圖像中每一個物體實例(instance)的唯一識別。eg. 在圖像中，不僅要區分出貓狗的不同，還要區分出相同類別中的不同實例(有兩種品種不同的狗)。意味著即使屬於相同類別，每個instance 仍有自己邊界和標籤</p>
<p>這邊先來看一下我們的主角Mask2Transformer在各影像分割領域皆大獲全勝呀
Figure 1. State-of-the-art segmentation architectures are typically specialized for each image segmentation task. Although recent work has proposed universal architectures that attempt all tasks and are competitive on semantic and panoptic segmentation, they struggle with segmenting instances. We propose Mask2Former, which, for the first time, outperforms the best specialized architec tures on three studied segmentation tasks on multiple datasets.在三個圖像分割任務（全景、實例和語義分割）中使用了四個流行的數據集（COCO、Cityscapes、ADE20K和Mapillary Vistas）來評估Mask2Former。在這些基準上，我們的通用架構Mask2Former在所有任務上都達到了與專用架構相當或更好的表現。Mask2Former創下了COCO全景分割57.8 PQ、COCO實例分割50.1 AP和ADE20K語義分割57.7 mIoU的最新最先進成果，這些成果均使用相同的架構實現。
Introduction
只因為semantic的不同，導致現在image segmentation又被分成三種不同任務(panoptic、instance or semantic segmentation)，在處理上，也為了他們的不同而個別設計出專用的處理架構
FCNs (Fully Convolutional Networks) 用於 semantic segmentation
Mask classification architecture 用於 instance segmentation</p>
<p>它們雖然在單一領域表現優異，但只要一跨出該領域，表現就會不好。</p>
<hr>
<p>缺乏針對其他任務的通用性
所以，第一個通用架構MaskFormer就誕生啦!
比較像是一個「一次做完所有事情」的模型，它會先把圖片分成不同區域，然後對每個區域進行分類和生成遮罩（mask）
能夠用同一個架構處理所有的分割任務（即通用圖像分割）
基於end-to-end 的集合預測目標（例如DETR），並能夠在不修改架構、損失函數或訓練過程的情況下成功應對多個任務
儘管使用通用的架構，但在訓練時仍會針對不同的任務和data set進行各別訓練</p>
<p>Q: 但為何MaskFromer推出後，最近的工作仍然專注於使用專門單一任務的架構，為何專門單一的架構尚未被通用架構所取代？
A: 儘管MaskFormer足夠靈活，可以應對任何分割任務，但在實際應用中，它們的表現仍然落後於最先進的專門架構。例如:
性能不足: 現有通用架構的最佳報告性能比專門架構的實例分割低9 AP以上
訓練困難: 通常需要更高階的硬體和更長的訓練時間。例如，訓練MaskFormer 需要300個epoch才能達到40.1 AP，而它只能在一個具有32G記憶體的GPU上處理單張圖像</p>
<p>相比之下，專門的Swin-HTC++只需72個epoch就能達到更好的性能。性能和訓練效率的問題阻礙了通用架構的部署。
第一代被打爆，沒關係 ! 那就來推出第二代啦
之後，Meta 提出了第二代改良版的，提出了一個名為「遮罩注意力遮罩變壓器（Mask2Former）」的通用圖像分割架構，它在不同的分割任務中均優於專門架構，同時在每個任務上仍易於訓練。
我們基於一個簡單的meta architecture，該架構組成包括一個 backbone feature extractor、pixel decoder 和 Transformer decoder
Figure 2. Mask2Former overview改良部分:</p>
<p>在Transformer decoder 中使用 mask-attention</p>
<blockquote>
<p>將注意力限制在圍繞預測分割區域的局部特徵中，這些分割區域可以是對象或區域，具體取決於分組的語義。相較於一般的 Transformer decoder 中使用的cross attention，它會關注圖像中的所有位置，而我們的mask-attention能夠加快收斂速度並改善性能
一般的 Self-attention : 會考慮到每個image結果 (包含後面的)</p></blockquote>
<p>Masked Self-attention: 只考慮自己(含)之前的結果</p>
<ol start="2">
<li>使用multi-scale high resolution features，有助於模型對小區域進行分割3. 他們提出了優化改進，eg. 交換self attention和cross attention的順序，使query feature 可學習，並移除dropout
Cross attention 介紹:</li>
</ol>
<p>假設輸入是一個special token，經過self-attention後
會得到一組vector，此向量會再乘上一個matrix，得到一個query(這個q:就是decoder產生的)
而Endoer這邊，會先產生出key: k1、k2、k3
這些key會與decoder這邊的query:q 相乘，計算出attention score，
得到alpha 1、2、3，這邊會再做正規化，所以對他用softmax進行正規化後
得到alpha 鋪浪 1、2、3，再與encoder的value相乘，計算出他們的權重
之後再將這些weighted sum相加，得到v，之後再丟入Fully connected network中做處理
一般的Transformer decoder 順序:</p>
<p>Self-Attention → Cross-Attention → Feed-Forward Network (FFN)</p>
<ol>
<li>Self-Attention（自注意力）： 首先，Transformer Decoder的每一層會先進行self-attention。這個機制讓decoder的每個查詢位置（query）可以關注到decoder內部其他位置的資訊，從而整合上下文資訊。例如，在語言模型中，這步驟使得句子的每個單詞能夠基於前面已生成的單詞來生成新的單詞</li>
<li>Cross-Attention（交叉注意力）： 完成self-attention後，Decoder會執行cross-attention，這一步是將decoder中的查詢（query）與encoder的輸出（key和value）進行匹配。Cross-attention的作用是將encoder的資訊整合到decoder中，使得decoder可以基於encoder提供的特徵來生成目標序列。例如，在機器翻譯中，這步驟讓decoder能夠根據源語句的編碼來生成目標語句。</li>
<li>Feed-Forward Network（前饋網路）： 最後，經過注意力機制後的特徵會通過一個前饋神經網路（Feed-Forward Network, FFN）進行進一步的非線性轉換和學習。這樣的結構設計能夠提升模型對特徵的表示能力，增強其泛化性能
此篇論文的Transformer decoder 順序:</li>
</ol>
<p>Cross-Attention → Self-Attention → Feed-Forward Network (FFN)</p>
<ol>
<li>Cross-Attention（交叉注意力）： 首先，decoder中的query features 會與來自 encoder（或pixel decoder）的圖像特徵進行cross-attention。這樣的設計可以讓查詢特徵先從圖像特徵中學習到有用的上下文資訊，形成對物件或區域的初步理解。</li>
<li>Self-Attention（自注意力）： 在cross-attention之後，query features再進行self-attention，這一步讓query features之間能夠互相交互學習，進一步整合資訊，增強查詢特徵的表示能力。這種設計能讓查詢特徵在獲取圖像資訊後進一步自我調整，使模型可以更加專注於圖像中不同區域的細節。</li>
<li>Feed-Forward Network（前饋網路）： 最後，經過cross-attention和self-attention後的查詢特徵會通過一個前饋神經網路（Feed-Forward Network, FFN），進行進一步的非線性轉換和學習。這一步驟進一步強化了查詢特徵的表達能力。</li>
<li>透過 random sample points 計算mask loss，以節省了3倍的訓練記憶體而不影響性能
所有這些改進在不增加計算量的情況下提升了性能，也使得訓練變得更加簡單，讓通用架構對於計算資源有限的用戶更為可及。
Related work
都在比較一些用於單一任務的模型
Specialized semantic segmentation architectures 專用語意分割架構</li>
</ol>
<p>將任務視為pixel分類問題，FCN-based (全卷積網路)的架構會獨立對每個pixel預測一個類別標籤。
此做法之後被發現對於上下文對於像素分類的精確非常重要，所以後續專注於設計context-module與self-attention variants 來提升精確度
2. Specialized instance segmentation architectures 專用實例分割架構
通常基於mask classification，會預測一組與單一類別標籤相關的binary mask。如Mask R-CNN會從偵測到的bounding boxed產生mask
後續方法專注於</p>
<ul>
<li>偵測到更精確的bounding boxed 或 尋找生成動態數量mask的新方法
eg. 使用Dynamic kernels 或 clustering algorithm
雖然任務表現有所提升，但仍缺乏通用性:從一任務跨到另一個任務上應用的話，表現就會不好與靈活性:導致重複研究。</li>
</ul>
<ol start="3">
<li>Panoptic segmentation 全景分割
目標是統一 semantic segmentation 與 instance segmentation 任務，
方法:</li>
</ol>
<ul>
<li>整合 semantic segmentation 與 instance segmentation 的最佳部分到單一框架中 or 設計一個平等的對待semantic segmentation 與 instance segmentation 的目標架構
全景分割架構的通用性有限，通常只在單一全景任務上表現良好，無法在其他任務中保持一致性能</li>
</ul>
<ol start="4">
<li>Universal architectures
ETR的出現讓遮罩分類架構具備通用性，可用於各種影像分割任務。
MaskFormer和K-Net等架構擴展了DETR的應用，但在特定任務上仍不如專門模型。
Mask2Former被視為首個在所有分割任務和數據集上超越專門架構表現的通用架構。
Masked-attention Mask Transformer
Mask2Former的框架:
Backbone:</li>
</ol>
<p>從 image 中提取最低解析度的特徵
2. Pixel decoder
從backbone 輸出的low-resolution feature 上採樣(upsampling)以生成每個pixel 的 高解析度嵌入
3. Transformer decoder
利用Transformer decoder 來處理影像特徵以處理物件查詢，最終的二元遮罩預測從每像素的嵌入和物件查詢中解碼出來
我們提出的Transformer decoder取代了標準解碼器。我們的Transformer decoder的關鍵在於改採用masked attention ，透過將cross-attention限制在每個查詢的預測遮罩的前景區域內，從而提取局部化特徵，而不是對整個特徵圖進行關注。
為了處理小物體，我們提出了多尺度策略，以利用高解析度特徵。它會依次將pixel decoder的feature pyramid 中的feature map 輸入到Transformer decoder 的 layer 中，並進行輪替(round-robin fashion)處理。最後，我們整合了優化改進，提高了模型性能而不增加額外計算。
3.1 Masked attention
標準Cross-Attention的計算</p>
<p>此公式可以把它想成一個「訊息聚焦」的過程。每一層的input 其實就是上一層的output，這邊的X_L 代表第L層的輸出特徵，他會累積每一層所產生的訊息，並逐層更新。
這邊會用到 Q: Query、K:Key、V:Value 這三個矩陣，模型會透過Query與key 的相似度計算(就是做內積)，來衡量每個 query 與 不同key 之間的相關性，這時候會得到一組&quot;相似度分數&quot;。
再透過softmax 將這些分數做正規化，讓他們變成attention weight(注意力權重)，這些權重代表模型認為最重要的部分。
這些attention weight 會再與矩陣V相乘，得到加權後的輸出值。最後再與原本的輸入 X_L-1 相加，這就是 residual connection(殘差連接)
。此做法可保留原始訊息，同時又融入新的資訊，讓模型不會忘記前一層的內容。
此公式的重點即是在原本的self-attention 中加入的一個 mask 來控制模型的注意力範圍，讓模型在計算注意力時可以忽略掉不重要的區域。M_L-1 這個矩陣會指定哪些區域需要被關注，這樣模型就能聚焦於真正有用的地方
當M_L-1 = 1時，該位置的值為0，表示此位置應被模型注意到
當M_L-1 = 0時，該位置的值為-無限，這樣softmax計算後會讓該位置的注意力權重變為0，完全忽視這個位置
3.2 High-resolution features
High-resolution features 可以提升模型性能，特別是對於小物體的檢測 。然而，使用 High-resolution features 的計算需求非常高。為了在控制計算量的同時又可以引入High-resolution features，論文提出了一種高效率的 multi-scale strategy。論文採用了包含low- and high-resolution fea tures的特徵金字塔（feature pyramid），並在每次只將multi-scale feature 中的其中一個resolution的輸入到Transformer decoder layer。
名詞解釋:</p>
<p>Multi-Scale Feature (多尺度特徵):</p>
<p>指從同一張image中提取出不同解析度的特徵表示。從整體影像中提取出不同解析度的特徵圖，形成一個特徵金字塔( Feature Pyramid）。這個Feature Pyramid 中每一層的特徵圖代表不同尺度的影像信息。
例如，在Mask2Former中，像素解碼器會生成1/32、1/16和1/8解析度的特徵圖。這些特徵圖是在不同解析度下進行處理，分別捕捉全局、局部和細節信息，並依次輸入到Transformer解碼器的不同層中。
這樣做的目的是讓模型在處理不同層的特徵時，能同時考慮到大範圍的上下文（低解析度）和小物體的細節（高解析度）</p>
<p>優化版的self-attention:
更換self-attention和cross-attention的順序：為了讓計算更有效，我們將self-attention和cross-attention的順序調換（即我們的新「masked attention」）。最初進入第一個self-attention層的查詢特徵是與影像無關的，並未包含來自影像的信號，因此此時應用self-attention無法有效增加信息。</p>
<ol start="2">
<li>使查詢特徵（X0​）變為可學習：我們使查詢特徵本身變為可學習（同時保留可學習的查詢位置嵌入），並且這些可學習的查詢特徵在用於Transformer解碼器進行遮罩預測（M0）之前會直接受到監督。我們發現這些可學習的查詢特徵類似於區域候選網路（Region Proposal Network, RPN），能夠產生遮罩候選。</li>
<li>移除dropout：我們發現dropout並非必要，且通常會降低性能。因此，我們在解碼器中完全移除了dropout。
3.3. Improving training efficiency
由於 High-resolution mask prediction 會占用大量memory。
eg. MaskFormer 僅能在32G記憶體的GPU中容納單一張影像
於是我們可以改透過隨機抽取k個點去計算其mask loss，而不需要計算整個mask loss，所以此篇論文在 matching 和 final loss計算中都使用這種random sampled points 的方式去計算mask loss。
Mask2Former透過此抽樣策略降低了計算需求，具體做法如下：
匹配損失中的統一抽樣：在構建代價矩陣時，對所有預測和真實遮罩使用相同的 K=12544K = 12544K=12544 個抽樣點，以減少計算量。
最終損失中的不同抽樣：在已配對的預測和真實遮罩之間的最終損失計算中，使用不同的抽樣點集合，並採用重要性抽樣來更精確地計算損失。</li>
</ol>
<p>這種策略有效地將訓練記憶體需求從18GB降至6GB，使Mask2Former在有限的計算資源下也能運行。
Experiments
Implement details:</p>
<p>這邊是他們loss function的設計，使用了兩種Loss function，分別是binary cross entropy、dice loss，整體的mask loss (Lmask) formula如圖所示:
lambda ce、dice代表權重
Lce代表cross entropy 的loss
Ldice 代表dice的loss</p>
<p>這邊是作者實驗得出的，權重係數都帶5</p>
<p>Training settings
先前有提到，即便是這種通用的架構，訓練時仍要個別訓練，所以論文就針對image segmentation 的各個類別去分別進行訓練</p>
<ol>
<li>
<p>Panoptic and Instance Segmentation
使用 Detectron2 框架，並遵循更新的 Mask R-CNN 基準設置，針對 COCO 資料集進行訓練
Optimizer：使用 AdamW和 調整 learning rate
Initial learning rate：設為 0.0001。
Weight decay：所有 backbone 設為 0.05。
learning rate調整：在backbone 上 Learning rate * 0.1、在training step 的 90% 和 95% 處將learning rate衰減 10 倍。
epoch 和 batch size：預設模型訓練 50 個 epoch，批次大小為 16。
數據擴增：使用 Large-Scale Jittering( LSJ），隨機縮放範圍從 0.1 到 2.0，裁切大小固定:1024×1024
推理設置：採用標準 Mask R-CNN 推理設置，將圖像短邊縮放至 800，長邊最大為 1333。
計算 FLOPs 和 fps：
FLOPs 是基於 100 張驗證圖像（不同大小的 COCO 圖像）平均計算。
每秒幀數（fps）是在 V100 GPU 上測量，批次大小為 1，取整個驗證集的平均運行時間（包括後處理時間）。</p>
</li>
<li>
<p>語義分割（Semantic Segmentation）
Semantic Segmentation 訓練設置上有些微差異，大多都遵循前面的設定，但有以下例外：</p>
</li>
</ol>
<p>學習率乘數：0.1 的學習率乘數應用於 CNN 和 Transformer 的骨幹網路，而非僅應用於 CNN 骨幹網路。
ResNet 和 Swin 骨幹：初始學習率設為 0.0001，權重衰減設為 0.05，而不是使用不同的學習率。</p>
<p>Results
Mask2Former的實驗部分展示了其在影像分割任務中的有效性
比較對象與數據集：</p>
<p>比較：Mask2Former被與最先進的專門影像分割模型進行了對比，包括DETR、Mask R-CNN、K-Net等模型。
數據集：使用了四個常見的影像分割數據集：COCO、ADE20K、Cityscapes和Mapillary Vistas，這些數據集包含了語義分割、實例分割和全景分割任務。</p>
<ol start="2">
<li>
<p>評估指標：
PQ (Panoptic Quality)：用於全景分割的指標。
APTh (Average Precision)：針對實例分割任務的「事物」類別的精確度。
mIoU (Mean Intersection over Union)：語義分割的標準指標，用於衡量模型在不同分割類別上的精度。</p>
</li>
<li>
<p>主要結果：
全景分割：</p>
</li>
</ol>
<p>在COCO數據集上，Mask2Former的表現超越了現有的模型，尤其是Swin-L的版本，PQ提升了5.1。這表明Mask2Former在不同骨幹網路上的表現均優於MaskFormer，且收斂速度更快。</p>
<p>實例分割：在COCO數據集上，Mask2Former使用較少的訓練迭代次數就超越了Mask R-CNN基線，並且在邊界AP（Boundary AP）上也取得了更高的性能，說明其遮罩預測在邊界上更加精細。
4. 消融實驗（Ablation Study）：</p>
<p>Masked Attention與高解析度特徵：移除masked attention或高解析度特徵會導致性能顯著下降，表明這些組件對於模型的性能提升至關重要。</p>
<p>實驗:
移除masked attention或高解析度特徵</p>
<p>-&gt; 導致性能顯著下降
=&gt; 表明這些組件對於模型的性能提升很重要 !
切換self-attention和cross-attention的順序、引入可學習(Learnable Queries)的查詢特徵並移除dropout:</p>
<p>=&gt; 均對性能有輕微提升，且不增加額外的計算量
不同的Pixel Decoder：MSDeformAttn(multi-scale de-formable attention Transformer) 在所有任務上表現最佳，表明其作為像素解碼器在各種分割任務中具有通用性。</p>
<ol start="5">
<li>
<p>記憶體優化：
通過抽樣點而不是整個遮罩來計算損失，將訓練過程的記憶體需求從18GB減少到6GB，同時保持性能不變。這一策略降低了計算資源需求，使得Mask2Former對於記憶體受限的環境更友好。</p>
</li>
<li>
<p>泛化能力：
Mask2Former在不同數據集上的表現均優異，展示了其作為通用影像分割模型的潛力，可用於多種影像分割任務。</p>
</li>
<li>
<p>限制：
Mask2Former雖然在多個任務上泛化良好，但仍然需要針對特定任務進行專門訓練。此外，模型在小物體分割上仍有改進空間，未能充分利用多尺度特徵。</p>
</li>
</ol>
<p>總結來說，Mask2Former在影像分割的主要任務上展現了良好的效果，並且通過多項創新設計提高了性能，減少了計算資源需求，但仍有進一步改進的空間。
Conclusion
Meta 提出了通用圖像分割的Mask2Former。基於一個簡單的元框架，改使用mask-attention的Transformer decoder
Mask2Former在四個流行的數據集上（COCO、Cityscapes、ADE20K和Mapillary Vistas），在全景分割、實例分割和語義分割三大圖像分割任務中都取得了最佳成績，已超越了每個任務所專用的模型，且易於訓練
與每個任務所專用的模型，Mask2Former節省了三倍的研究精力，對運算資源有限的用戶也十分友善</p>

        </div>

        
        
      </div>
    </div>
  </div>
</section>





<script>
  var indexURL = "https://yifunlin.github.io/index.json"
</script>

<!-- JS Plugins -->

<script src="https://yifunlin.github.io/plugins/jQuery/jquery.min.js"></script>

<script src="https://yifunlin.github.io/plugins/bootstrap/bootstrap.min.js"></script>

<script src="https://yifunlin.github.io/plugins/slick/slick.min.js"></script>

<script src="https://yifunlin.github.io/plugins/venobox/venobox.min.js"></script>

<script src="https://yifunlin.github.io/plugins/search/fuse.min.js"></script>

<script src="https://yifunlin.github.io/plugins/search/mark.js"></script>

<script src="https://yifunlin.github.io/plugins/search/search.js"></script>

<!-- Main Script -->

<script src="https://yifunlin.github.io/js/script.min.js"></script>




<script src="https://cdnjs.cloudflare.com/ajax/libs/js-cookie/2.2.1/js.cookie.min.js"></script>
<div id="js-cookie-box" class="cookie-box cookie-box-hide">
	This site uses cookies. By continuing to use this website, you agree to their use. <span id="js-cookie-button" class="btn btn-sm btn-primary ml-2">I Accept</span>
</div>
<script>
	(function ($) {
		const cookieBox = document.getElementById('js-cookie-box');
		const cookieButton = document.getElementById('js-cookie-button');
		if (!Cookies.get('cookie-box')) {
			cookieBox.classList.remove('cookie-box-hide');
			cookieButton.onclick = function () {
				Cookies.set('cookie-box', true, {
					expires:  2 
				});
				cookieBox.classList.add('cookie-box-hide');
			};
		}
	})(jQuery);
</script>


<style>
.cookie-box {
  position: fixed;
  left: 0;
  right: 0;
  bottom: 0;
  text-align: center;
  z-index: 9999;
  padding: 1rem 2rem;
  background: rgb(71, 71, 71);
  transition: all .75s cubic-bezier(.19, 1, .22, 1);
  color: #fdfdfd;
}

.cookie-box-hide {
  display: none;
}
</style>
</body>
</html>